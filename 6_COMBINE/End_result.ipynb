{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a113ce24-0180-4fb9-81fe-732355fcecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Started MCP event loop\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": initializing with: {'url': 'http://127.0.0.1:8000/mcp'}\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": Pre-validating authentication\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": Authentication validation passed: 307\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": testing Streamable HTTP support for http://127.0.0.1:8000/mcp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": detected Streamable HTTP transport support\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:mcp.client.streamable_http:Received session ID: 9a8ffaf498f342b1ab2ba64b163e253b\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-06-18\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": connected\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp/ \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"local_http\": 4 tool(s) available:\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- train_logistics_model\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- predict_next_shipment\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- echo\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- do_web_request\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP servers initialized: 4 tool(s) available in total\n",
      "INFO:__main__:  - Added MCP tool: train_logistics_model\n",
      "INFO:__main__:  - Added MCP tool: predict_next_shipment\n",
      "INFO:__main__:  - Added MCP tool: echo\n",
      "INFO:__main__:  - Added MCP tool: do_web_request\n",
      "INFO:__main__:✅ Loaded 4 MCP tools\n",
      "INFO:__main__:✅ Loaded 6 tools\n",
      "INFO:__main__:✅ Created agent: Python Coder\n",
      "INFO:__main__:✅ Created agent: Data Analyst\n",
      "INFO:__main__:✅ Created agent: Assistant\n",
      "INFO:__main__:✅ Dashboard initialized successfully!\n",
      "/tmp/ipykernel_3309/3952706411.py:485: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=600)\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7862/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7862/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://0412ea84ade9c53ece.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://0412ea84ade9c53ece.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0412ea84ade9c53ece.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Started MCP event loop\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": initializing with: {'url': 'http://localhost:8000/mcp'}\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": Pre-validating authentication\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": Authentication validation passed: 307\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": testing Streamable HTTP support for http://localhost:8000/mcp\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": detected Streamable HTTP transport support\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:mcp.client.streamable_http:Received session ID: 1130dd800ad54ddabdb60cacf05b89c9\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-06-18\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": connected\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp/ \"HTTP/1.1 202 Accepted\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8000/mcp/ \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"example_server\": 4 tool(s) available:\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- train_logistics_model\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- predict_next_shipment\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- echo\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:- do_web_request\n",
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP servers initialized: 4 tool(s) available in total\n",
      "INFO:__main__:  - Added MCP tool: train_logistics_model\n",
      "INFO:__main__:  - Added MCP tool: predict_next_shipment\n",
      "INFO:__main__:  - Added MCP tool: echo\n",
      "INFO:__main__:  - Added MCP tool: do_web_request\n",
      "INFO:__main__:✅ Loaded 4 MCP tools\n",
      "INFO:__main__:✅ Loaded 6 tools\n",
      "INFO:__main__:✅ Created agent: Python Coder\n",
      "INFO:__main__:✅ Created agent: Data Analyst\n",
      "INFO:__main__:✅ Created agent: Assistant\n",
      "INFO:__main__:✅ Dashboard initialized successfully!\n",
      "/tmp/ipykernel_3309/3952706411.py:485: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=600)\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7863/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7863/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://f00398d56d3213c5c8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://f00398d56d3213c5c8.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f00398d56d3213c5c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI have the following tools available:\n",
      "\n",
      "1. **python_repl**: Execute Python code for data analysis and visualization.\n",
      "2. **bash_command**: Execute shell commands.\n",
      "3. **train_logistics_model**: Train and persist a linear model on (timestamp, value) pairs.\n",
      "4. **predict_next_shipment**: Load a saved model and predict the value for the next timestamp.\n",
      "5. **echo**: Return input verbatim.\n",
      "6. **do_web_request**: Perform a GET request to a given URL and return the response text (first 2000 chars).\n",
      "\n",
      "These tools can be used for various data analysis, modeling, and web request tasks. If you need assistance with any specific task, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/audio/speech \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import threading\n",
    "from typing import Optional, Tuple\n",
    "import io\n",
    "from openai import OpenAI\n",
    "# Import necessary LangChain components\n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools import Tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_community.tools import ShellTool\n",
    "\n",
    "import wave\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "\n",
    "\n",
    "import dotenv\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Apply nest_asyncio to handle async issues\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up logging so we can see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Initialize OpenAI client (using v1+ Python library interface)\n",
    "client = OpenAI()\n",
    "\n",
    "class SimpleAudioHandler:\n",
    "    \"\"\"Handles speech-to-text and text-to-speech using OpenAI's v1 audio endpoints.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.can_hear = True\n",
    "        self.can_speak = True\n",
    "\n",
    "    def listen(self, audio_data: Tuple[int, np.ndarray]) -> str:\n",
    "        \"\"\"Transcribe audio_data using OpenAI's Whisper-based endpoint.\"\"\"\n",
    "        if not self.can_hear or audio_data is None:\n",
    "            return \"\"\n",
    "\n",
    "        sample_rate, audio_array = audio_data\n",
    "        # Pack numpy array into WAV bytes\n",
    "        buf = io.BytesIO()\n",
    "        buf.name = \"audio.wav\"  # ensure correct file extension for OpenAI\n",
    "        with wave.open(buf, 'wb') as wav_file:\n",
    "            wav_file.setnchannels(1)\n",
    "            wav_file.setsampwidth(2)\n",
    "            wav_file.setframerate(sample_rate)\n",
    "            if audio_array.dtype != np.int16:\n",
    "                audio_array = (audio_array * 32767).astype(np.int16)\n",
    "            wav_file.writeframes(audio_array.tobytes())\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Call new v1 API\n",
    "        response = client.audio.transcriptions.create(\n",
    "            file=buf,\n",
    "            model=\"gpt-4o-transcribe\"\n",
    "        )\n",
    "        return getattr(response, 'text', '')\n",
    "\n",
    "    def speak(self, text: str) -> Optional[Tuple[int, np.ndarray]]:\n",
    "        \"\"\"Generate speech audio from text via OpenAI's TTS endpoint.\"\"\"\n",
    "        if not self.can_speak or not text:\n",
    "            return None\n",
    "\n",
    "        # Synthesize speech using v1 API\n",
    "        response = client.audio.speech.create(\n",
    "            model=\"gpt-4o-mini-tts\",\n",
    "            input=text,\n",
    "            voice=\"ash\"\n",
    "        )\n",
    "        audio_bytes = response\n",
    "        if not audio_bytes:\n",
    "            return None\n",
    "     # Extract raw bytes\n",
    "        if hasattr(response, 'content'):\n",
    "            audio_bytes = response.content  # typical for HTTPX-like response\n",
    "        elif hasattr(response, 'read'):\n",
    "            audio_bytes = response.read()   # fallback if streaming\n",
    "        else:\n",
    "            # assume it's already bytes\n",
    "            audio_bytes = response  # type: ignore\n",
    "\n",
    "        # Ensure bytes-like\n",
    "        if not isinstance(audio_bytes, (bytes, bytearray)):\n",
    "            try:\n",
    "                audio_bytes = bytes(audio_bytes)\n",
    "            except Exception:\n",
    "                raise TypeError(f\"Could not convert TTS response to bytes: {type(audio_bytes)}\")\n",
    "\n",
    "        # Load WAV bytes into numpy\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "        tmp.write(audio_bytes)\n",
    "        tmp.close()\n",
    "        # Return filepath for Gradio Audio (type=\"filepath\")\n",
    "        return tmp.name\n",
    "\n",
    "\n",
    "class SimpleMultiAgentDashboard:\n",
    "    \"\"\"A simplified multi-agent dashboard for students to learn from\"\"\"\n",
    "    \n",
    "    def __init__(self, mcp_servers: Dict, agent_configs: List[Dict]):\n",
    "        \"\"\"Initialize the dashboard with servers and agent configurations\"\"\"\n",
    "        self.mcp_servers = mcp_servers\n",
    "        self.agent_configs = agent_configs\n",
    "        self.agents = {}  # Will store our created agents\n",
    "        self.tools = []   # Will store all available tools\n",
    "        self.max_iterations = 5  # How many times an agent can use tools\n",
    "        self.audio = SimpleAudioHandler()  # For voice input/output\n",
    "        \n",
    "        # MCP event loop for async operations\n",
    "        self.mcp_loop = None\n",
    "        self.mcp_thread = None\n",
    "        self.cleanup_func = None\n",
    "        \n",
    "        # Initialize everything\n",
    "        self._setup_tools()\n",
    "        self._create_agents()\n",
    "        logger.info(\"✅ Dashboard initialized successfully!\")\n",
    "        \n",
    "        # Register cleanup on exit\n",
    "        import atexit\n",
    "        atexit.register(self._cleanup)\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Clean up resources when shutting down\"\"\"\n",
    "        logger.info(\"Cleaning up...\")\n",
    "        \n",
    "        # Clean up MCP resources\n",
    "        if self.cleanup_func and self.mcp_loop:\n",
    "            try:\n",
    "                future = asyncio.run_coroutine_threadsafe(\n",
    "                    self.cleanup_func(),\n",
    "                    self.mcp_loop\n",
    "                )\n",
    "                future.result(timeout=5)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup error: {e}\")\n",
    "        \n",
    "        # Stop MCP event loop\n",
    "        if self.mcp_loop:\n",
    "            self.mcp_loop.call_soon_threadsafe(self.mcp_loop.stop)\n",
    "            if self.mcp_thread:\n",
    "                self.mcp_thread.join(timeout=2)\n",
    "    \n",
    "    def _setup_tools(self):\n",
    "        \"\"\"Set up all the tools our agents can use\"\"\"\n",
    "        \n",
    "        # 1. Python REPL Tool - for running Python code\n",
    "        python_tool = PythonREPLTool()\n",
    "        # Wrap it to handle different input formats\n",
    "        wrapped_python = Tool(\n",
    "            name=\"python_repl\",\n",
    "            description=\"Execute Python code. Returns the output.\",\n",
    "            func=lambda code: python_tool.run(code if isinstance(code, str) else str(code))\n",
    "        )\n",
    "        self.tools.append(wrapped_python)\n",
    "        \n",
    "        # 2. Bash/Shell Tool - for running system commands\n",
    "        bash_tool = Tool(\n",
    "            name=\"bash_command\",\n",
    "            description=\"Execute shell commands.\",\n",
    "            func=ShellTool().run\n",
    "        )\n",
    "        self.tools.append(bash_tool)\n",
    "        \n",
    "        # 3. MCP Tools - if configured\n",
    "        if self.mcp_servers:\n",
    "            self._load_mcp_tools()\n",
    "        \n",
    "        logger.info(f\"✅ Loaded {len(self.tools)} tools\")\n",
    "    \n",
    "    def _load_mcp_tools(self):\n",
    "        \"\"\"Load MCP (Model Context Protocol) tools\"\"\"\n",
    "        try:\n",
    "            # Create a dedicated event loop for MCP operations\n",
    "            self.mcp_loop = asyncio.new_event_loop()\n",
    "            \n",
    "            # Run the loop in a separate thread\n",
    "            self.mcp_thread = threading.Thread(\n",
    "                target=self.mcp_loop.run_forever,\n",
    "                daemon=True,\n",
    "                name=\"MCP-Thread\"\n",
    "            )\n",
    "            self.mcp_thread.start()\n",
    "            logger.info(\"Started MCP event loop\")\n",
    "            \n",
    "            # Load MCP tools in the dedicated loop\n",
    "            future = asyncio.run_coroutine_threadsafe(\n",
    "                convert_mcp_to_langchain_tools(self.mcp_servers),\n",
    "                self.mcp_loop\n",
    "            )\n",
    "            \n",
    "            # Get the tools and cleanup function\n",
    "            mcp_tools, self.cleanup_func = future.result(timeout=30)\n",
    "            \n",
    "            # Wrap each MCP tool to work with our system\n",
    "            for tool in mcp_tools:\n",
    "                wrapped_tool = self._wrap_mcp_tool(tool)\n",
    "                self.tools.append(wrapped_tool)\n",
    "                logger.info(f\"  - Added MCP tool: {tool.name}\")\n",
    "            \n",
    "            logger.info(f\"✅ Loaded {len(mcp_tools)} MCP tools\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Could not load MCP tools: {e}\")\n",
    "            logger.info(\"Continuing without MCP tools...\")\n",
    "    \n",
    "    def _wrap_mcp_tool(self, mcp_tool):\n",
    "        \"\"\"Wrap an MCP tool to handle async operations properly\"\"\"\n",
    "        def sync_wrapper(input_data):\n",
    "            \"\"\"Run the async MCP tool synchronously\"\"\"\n",
    "            try:\n",
    "                # Handle different input formats\n",
    "                if isinstance(input_data, dict):\n",
    "                    final_input = input_data\n",
    "                else:\n",
    "                    # Try to create a dict with common parameter names\n",
    "                    final_input = {\"text\": str(input_data)}\n",
    "                \n",
    "                logger.info(f\"Calling MCP tool '{mcp_tool.name}' with: {final_input}\")\n",
    "                \n",
    "                # Run the async tool in the MCP event loop\n",
    "                future = asyncio.run_coroutine_threadsafe(\n",
    "                    mcp_tool.ainvoke(final_input),\n",
    "                    self.mcp_loop\n",
    "                )\n",
    "                \n",
    "                # Wait for result\n",
    "                result = future.result(timeout=30)\n",
    "                \n",
    "                # Format the result\n",
    "                if isinstance(result, dict):\n",
    "                    return json.dumps(result, indent=2)\n",
    "                else:\n",
    "                    return str(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"MCP tool error: {e}\")\n",
    "                return f\"Error using tool: {str(e)}\"\n",
    "        \n",
    "        # Create a new Tool with the wrapper\n",
    "        return Tool(\n",
    "            name=mcp_tool.name,\n",
    "            description=mcp_tool.description,\n",
    "            func=sync_wrapper\n",
    "        )\n",
    "    \n",
    "    def _create_agents(self):\n",
    "        \"\"\"Create an agent for each configuration\"\"\"\n",
    "        for config in self.agent_configs:\n",
    "            agent_name = config[\"name\"]\n",
    "            \n",
    "            # Create the language model\n",
    "            llm = ChatOpenAI(\n",
    "                model=config.get(\"model\", \"gpt-4o\"),\n",
    "                temperature=config.get(\"temperature\", 0.7)\n",
    "            )\n",
    "            \n",
    "            # Select which tools this agent can use\n",
    "            if agent_name == \"Python Coder\":\n",
    "                # Only give code execution tools\n",
    "                agent_tools = [t for t in self.tools if t.name in [\"python_repl\", \"bash_command\"]]\n",
    "            elif agent_name == \"Data Analyst\":\n",
    "                # Give all tools\n",
    "                agent_tools = self.tools\n",
    "            else:\n",
    "                # Give everything except code execution\n",
    "                agent_tools = [t for t in self.tools if t.name not in [\"python_repl\", \"bash_command\"]]\n",
    "            \n",
    "            # Create the agent's instructions\n",
    "            system_message = self._create_system_message(agent_name, agent_tools)\n",
    "            \n",
    "            # Create the prompt template\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_message),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "            ])\n",
    "            \n",
    "            # Create the agent\n",
    "            agent = create_tool_calling_agent(llm, agent_tools, prompt)\n",
    "            \n",
    "            # Create the executor (handles tool execution)\n",
    "            executor = AgentExecutor(\n",
    "                agent=agent,\n",
    "                tools=agent_tools,\n",
    "                verbose=True,  # Show what the agent is doing\n",
    "                max_iterations=self.max_iterations\n",
    "            )\n",
    "            \n",
    "            # Store the agent\n",
    "            self.agents[agent_name] = {\n",
    "                \"executor\": executor,\n",
    "                \"tools\": agent_tools,\n",
    "                \"config\": config\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"✅ Created agent: {agent_name}\")\n",
    "    \n",
    "    def _create_system_message(self, agent_name: str, tools: List) -> str:\n",
    "        \"\"\"Create instructions for each agent type\"\"\"\n",
    "        # List all available tools\n",
    "        tool_list = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "        \n",
    "        if agent_name == \"Python Coder\":\n",
    "            return f\"\"\"You are a Python programming assistant.\n",
    "\n",
    "Available tools:\n",
    "{tool_list}\n",
    "\n",
    "Instructions:\n",
    "- Write clean Python code\n",
    "- Use python_repl to execute code and show results\n",
    "- Always use print() to display outputs\n",
    "- Use bash_command for system operations\"\"\"\n",
    "        \n",
    "        elif agent_name == \"Data Analyst\":\n",
    "            return f\"\"\"You are a data analysis assistant.\n",
    "\n",
    "Available tools:\n",
    "{tool_list}\n",
    "\n",
    "Instructions:\n",
    "- Use python_repl for data analysis and visualization\n",
    "- Always print() results, dataframes, and statistics\n",
    "- Use appropriate tools for different tasks\n",
    "- Explain your findings clearly\"\"\"\n",
    "        \n",
    "        else:\n",
    "            return f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Available tools:\n",
    "{tool_list}\n",
    "\n",
    "Instructions:\n",
    "- Use tools to help answer questions\n",
    "- Provide clear and helpful responses\n",
    "- Explain what you're doing\"\"\"\n",
    "    \n",
    "    def chat_with_agent(self, message: str, agent_name: str, history: List) -> str:\n",
    "        \"\"\"Send a message to an agent and get a response\"\"\"\n",
    "        if agent_name not in self.agents:\n",
    "            return f\"Sorry, I don't know an agent named {agent_name}\"\n",
    "        \n",
    "        try:\n",
    "            # Get the agent's executor\n",
    "            executor = self.agents[agent_name][\"executor\"]\n",
    "            \n",
    "            # Convert history to LangChain format\n",
    "            chat_history = []\n",
    "            for msg in history[-10:]:  # Only use last 10 messages\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    chat_history.append(HumanMessage(content=msg[\"content\"]))\n",
    "                else:\n",
    "                    chat_history.append(AIMessage(content=msg[\"content\"]))\n",
    "            \n",
    "            # Get the response\n",
    "            result = executor.invoke({\n",
    "                \"input\": message,\n",
    "                \"chat_history\": chat_history\n",
    "            })\n",
    "            \n",
    "            # Return the agent's response\n",
    "            return result[\"output\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def process_voice_or_text(self, audio_input, text_input, agent_name, history, speak_response):\n",
    "        \"\"\"Process either voice or text input and optionally speak the response\"\"\"\n",
    "        # Get the message from voice or text\n",
    "        if audio_input is not None:\n",
    "            message = self.audio.listen(audio_input)\n",
    "            if not message:\n",
    "                return None, history, \"\", \"Could not understand audio\"\n",
    "        else:\n",
    "            message = text_input\n",
    "        \n",
    "        if not message.strip():\n",
    "            return None, history, \"\", \"No message detected\"\n",
    "        \n",
    "        # Get response from agent\n",
    "        response = self.chat_with_agent(message, agent_name, history)\n",
    "        \n",
    "        # Generate voice response if requested\n",
    "        voice_response = None\n",
    "        if speak_response and self.audio.can_speak and len(response) < 1000:\n",
    "            voice_response = self.audio.speak(response[:500])  # Limit length for TTS\n",
    "        \n",
    "        # Update history\n",
    "        new_history = history + [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        return voice_response, new_history, \"\", message\n",
    "    \n",
    "    def create_gradio_interface(self):\n",
    "        \"\"\"Create the Gradio web interface\"\"\"\n",
    "        with gr.Blocks(title=\"Multi-Agent Assistant\", theme=gr.themes.Soft()) as interface:\n",
    "            # Title\n",
    "            gr.Markdown(\"# 🤖 Multi-Agent Voice Assistant with Tools\")\n",
    "            gr.Markdown(\"Chat with different AI agents using voice or text!\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Left column - Controls\n",
    "                with gr.Column(scale=1):\n",
    "                    # Agent selector\n",
    "                    agent_dropdown = gr.Dropdown(\n",
    "                        choices=[config[\"name\"] for config in self.agent_configs],\n",
    "                        value=self.agent_configs[0][\"name\"],\n",
    "                        label=\"Select Agent\"\n",
    "                    )\n",
    "                    \n",
    "                    # Max iterations slider\n",
    "                    iterations_slider = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=10,\n",
    "                        value=self.max_iterations,\n",
    "                        step=1,\n",
    "                        label=\"Max Tool Uses\",\n",
    "                        info=\"How many times the agent can use tools\"\n",
    "                    )\n",
    "                    \n",
    "                    # Agent info display\n",
    "                    agent_info = gr.Markdown(\"\")\n",
    "                    \n",
    "                    # Voice input (if available)\n",
    "                    if self.audio.can_hear:\n",
    "                        audio_input = gr.Audio(\n",
    "                            sources=[\"microphone\"],\n",
    "                            type=\"numpy\",\n",
    "                            label=\"🎤 Voice Input (click to record)\"\n",
    "                        )\n",
    "                    else:\n",
    "                        audio_input = None\n",
    "                        gr.Markdown(\"*Voice input not available - install SpeechRecognition*\")\n",
    "                    \n",
    "                    # Text input\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"Text Input\",\n",
    "                        placeholder=\"Or type your message here...\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    \n",
    "                    # Send button\n",
    "                    send_button = gr.Button(\"Send\", variant=\"primary\")\n",
    "                    \n",
    "                    # Voice output toggle (if available)\n",
    "                    if self.audio.can_speak:\n",
    "                        speak_toggle = gr.Checkbox(\n",
    "                            label=\"🔊 Speak Response\",\n",
    "                            value=True\n",
    "                        )\n",
    "                        audio_output = gr.Audio(\n",
    "                            label=\"Voice Response\",\n",
    "                            type=\"numpy\",\n",
    "                            autoplay=True\n",
    "                        )\n",
    "                    else:\n",
    "                        speak_toggle = gr.State(False)\n",
    "                        audio_output = None\n",
    "                        gr.Markdown(\"*Voice output not available - install edge-tts*\")\n",
    "                \n",
    "                # Right column - Chat\n",
    "                with gr.Column(scale=2):\n",
    "                    # Chat history display\n",
    "                    chatbot = gr.Chatbot(label=\"Conversation\", height=600)\n",
    "                    \n",
    "                    # Last input display\n",
    "                    last_input = gr.Textbox(label=\"Last Input\", interactive=False)\n",
    "                    \n",
    "                    # Clear button\n",
    "                    clear_button = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            # Hidden state to store conversation history\n",
    "            history_state = gr.State([])\n",
    "            \n",
    "            # Function to update agent info\n",
    "            def update_agent_info(agent_name):\n",
    "                \"\"\"Show information about the selected agent\"\"\"\n",
    "                if agent_name in self.agents:\n",
    "                    tools = self.agents[agent_name][\"tools\"]\n",
    "                    tool_names = [tool.name for tool in tools]\n",
    "                    return f\"**{agent_name}**\\n\\nAvailable tools: {', '.join(tool_names)}\"\n",
    "                return \"\"\n",
    "            \n",
    "            # Function to update max iterations\n",
    "            def update_iterations(value):\n",
    "                \"\"\"Update the max iterations for all agents\"\"\"\n",
    "                self.max_iterations = value\n",
    "                for agent_data in self.agents.values():\n",
    "                    agent_data[\"executor\"].max_iterations = value\n",
    "            \n",
    "            # Function to clear chat\n",
    "            def clear_chat():\n",
    "                \"\"\"Clear the conversation\"\"\"\n",
    "                return [], [], \"\", \"\"\n",
    "            \n",
    "            # Connect all the interface elements\n",
    "            \n",
    "            # Update agent info when selection changes\n",
    "            agent_dropdown.change(\n",
    "                update_agent_info,\n",
    "                inputs=[agent_dropdown],\n",
    "                outputs=[agent_info]\n",
    "            )\n",
    "            \n",
    "            # Update iterations when slider changes\n",
    "            iterations_slider.change(\n",
    "                update_iterations,\n",
    "                inputs=[iterations_slider]\n",
    "            )\n",
    "            \n",
    "            # Process message when send button clicked\n",
    "            send_button.click(\n",
    "                self.process_voice_or_text,\n",
    "                inputs=[\n",
    "                    audio_input if audio_input else gr.State(None),\n",
    "                    text_input,\n",
    "                    agent_dropdown,\n",
    "                    history_state,\n",
    "                    speak_toggle\n",
    "                ],\n",
    "                outputs=[\n",
    "                    audio_output if audio_output else gr.State(None),\n",
    "                    history_state,\n",
    "                    text_input,\n",
    "                    last_input\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Update chat display when history changes\n",
    "            history_state.change(\n",
    "                lambda h: [(m[\"content\"], None) if m[\"role\"] == \"user\" \n",
    "                          else (None, m[\"content\"]) for m in h],\n",
    "                inputs=[history_state],\n",
    "                outputs=[chatbot]\n",
    "            )\n",
    "            \n",
    "            # Clear everything when clear button clicked\n",
    "            clear_button.click(\n",
    "                clear_chat,\n",
    "                outputs=[chatbot, history_state, text_input, last_input]\n",
    "            )\n",
    "            \n",
    "            # Initialize agent info on load\n",
    "            interface.load(\n",
    "                lambda: update_agent_info(self.agent_configs[0][\"name\"]),\n",
    "                outputs=[agent_info]\n",
    "            )\n",
    "        \n",
    "        return interface\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure MCP servers (optional)\n",
    "    MCP_SERVERS = {\n",
    "        \"local_http\": {\"url\": \"http://127.0.0.1:8000/mcp\"}\n",
    "    }\n",
    "    \n",
    "    # Configure agents\n",
    "    AGENT_CONFIGS = [\n",
    "        {\n",
    "            \"name\": \"Python Coder\",\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.2,\n",
    "            \"description\": \"Helps with Python programming\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Data Analyst\", \n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"description\": \"Helps with data analysis\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Assistant\",\n",
    "            \"model\": \"gpt-4o\", \n",
    "            \"temperature\": 0.7,\n",
    "            \"description\": \"General helpful assistant\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create and launch the dashboard\n",
    "    dashboard = SimpleMultiAgentDashboard(MCP_SERVERS, AGENT_CONFIGS)\n",
    "    app = dashboard.create_gradio_interface()\n",
    "    app.launch(share=True)\n",
    "    \"\"\"A simplified multi-agent dashboard for students to learn from\"\"\"\n",
    "    \n",
    "    def __init__(self, mcp_servers: Dict, agent_configs: List[Dict]):\n",
    "        \"\"\"Initialize the dashboard with servers and agent configurations\"\"\"\n",
    "        self.mcp_servers = mcp_servers\n",
    "        self.agent_configs = agent_configs\n",
    "        self.agents = {}  # Will store our created agents\n",
    "        self.tools = []   # Will store all available tools\n",
    "        self.max_iterations = 5  # How many times an agent can use tools\n",
    "        \n",
    "        # Initialize everything\n",
    "        self._setup_tools()\n",
    "        self._create_agents()\n",
    "        logger.info(\"✅ Dashboard initialized successfully!\")\n",
    "    \n",
    "    def _setup_tools(self):\n",
    "        \"\"\"Set up all the tools our agents can use\"\"\"\n",
    "        \n",
    "        # 1. Python REPL Tool - for running Python code\n",
    "        python_tool = PythonREPLTool()\n",
    "        # Wrap it to handle different input formats\n",
    "        wrapped_python = Tool(\n",
    "            name=\"python_repl\",\n",
    "            description=\"Execute Python code. Returns the output.\",\n",
    "            func=lambda code: python_tool.run(code if isinstance(code, str) else str(code))\n",
    "        )\n",
    "        self.tools.append(wrapped_python)\n",
    "        \n",
    "        # 2. Bash/Shell Tool - for running system commands\n",
    "        bash_tool = Tool(\n",
    "            name=\"bash_command\",\n",
    "            description=\"Execute shell commands.\",\n",
    "            func=ShellTool().run\n",
    "        )\n",
    "        self.tools.append(bash_tool)\n",
    "        \n",
    "        # 3. MCP Tools - if configured\n",
    "        if self.mcp_servers:\n",
    "            self._load_mcp_tools()\n",
    "        \n",
    "        logger.info(f\"✅ Loaded {len(self.tools)} tools\")\n",
    "    \n",
    "    def _load_mcp_tools(self):\n",
    "        \"\"\"Load MCP (Model Context Protocol) tools\"\"\"\n",
    "        try:\n",
    "            # Create an event loop for async operations\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            \n",
    "            # Load MCP tools\n",
    "            mcp_tools, cleanup = loop.run_until_complete(\n",
    "                convert_mcp_to_langchain_tools(self.mcp_servers)\n",
    "            )\n",
    "            \n",
    "            # Add each MCP tool to our tools list\n",
    "            for tool in mcp_tools:\n",
    "                self.tools.append(tool)\n",
    "                logger.info(f\"  - Added MCP tool: {tool.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Could not load MCP tools: {e}\")\n",
    "            logger.info(\"Continuing without MCP tools...\")\n",
    "    \n",
    "    def _create_agents(self):\n",
    "        \"\"\"Create an agent for each configuration\"\"\"\n",
    "        for config in self.agent_configs:\n",
    "            agent_name = config[\"name\"]\n",
    "            \n",
    "            # Create the language model\n",
    "            llm = ChatOpenAI(\n",
    "                model=config.get(\"model\", \"gpt-4o\"),\n",
    "                temperature=config.get(\"temperature\", 0.7)\n",
    "            )\n",
    "            \n",
    "            # Select which tools this agent can use\n",
    "            if agent_name == \"Python Coder\":\n",
    "                # Only give code execution tools\n",
    "                agent_tools = [t for t in self.tools if t.name in [\"python_repl\", \"bash_command\"]]\n",
    "            elif agent_name == \"Data Analyst\":\n",
    "                # Give all tools\n",
    "                agent_tools = self.tools\n",
    "            else:\n",
    "                # Give everything except code execution\n",
    "                agent_tools = [t for t in self.tools if t.name not in [\"python_repl\", \"bash_command\"]]\n",
    "            \n",
    "            # Create the agent's instructions\n",
    "            system_message = self._create_system_message(agent_name, agent_tools)\n",
    "            \n",
    "            # Create the prompt template\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_message),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "            ])\n",
    "            \n",
    "            # Create the agent\n",
    "            agent = create_tool_calling_agent(llm, agent_tools, prompt)\n",
    "            \n",
    "            # Create the executor (handles tool execution)\n",
    "            executor = AgentExecutor(\n",
    "                agent=agent,\n",
    "                tools=agent_tools,\n",
    "                verbose=True,  # Show what the agent is doing\n",
    "                max_iterations=self.max_iterations\n",
    "            )\n",
    "            \n",
    "            # Store the agent\n",
    "            self.agents[agent_name] = {\n",
    "                \"executor\": executor,\n",
    "                \"tools\": agent_tools,\n",
    "                \"config\": config\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"✅ Created agent: {agent_name}\")\n",
    "    \n",
    "    def _create_system_message(self, agent_name: str, tools: List) -> str:\n",
    "        \"\"\"Create instructions for each agent type\"\"\"\n",
    "        # List all available tools\n",
    "        tool_list = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "        \n",
    "        if agent_name == \"Python Coder\":\n",
    "            return f\"\"\"You are a Python programming assistant.\n",
    "\n",
    "Available tools:\n",
    "{tool_list}\n",
    "\n",
    "Instructions:\n",
    "- Write clean Python code\n",
    "- Use python_repl to execute code and show results\n",
    "- Always use print() to display outputs\n",
    "- Use bash_command for system operations\"\"\"\n",
    "        \n",
    "        elif agent_name == \"Data Analyst\":\n",
    "            return f\"\"\"You are a data analysis assistant.\n",
    "\n",
    "Available tools:\n",
    "{tool_list}\n",
    "\n",
    "Instructions:\n",
    "- Use python_repl for data analysis and visualization\n",
    "- Always print() results, dataframes, and statistics\n",
    "- Use appropriate tools for different tasks\n",
    "- Explain your findings clearly\"\"\"\n",
    "        \n",
    "        else:\n",
    "            return f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Available tools:\n",
    "{tool_list}\n",
    "\n",
    "Instructions:\n",
    "- Use tools to help answer questions\n",
    "- Provide clear and helpful responses\n",
    "- Explain what you're doing\"\"\"\n",
    "    \n",
    "    def chat_with_agent(self, message: str, agent_name: str, history: List) -> str:\n",
    "        \"\"\"Send a message to an agent and get a response\"\"\"\n",
    "        if agent_name not in self.agents:\n",
    "            return f\"Sorry, I don't know an agent named {agent_name}\"\n",
    "        \n",
    "        try:\n",
    "            # Get the agent's executor\n",
    "            executor = self.agents[agent_name][\"executor\"]\n",
    "            \n",
    "            # Convert history to LangChain format\n",
    "            chat_history = []\n",
    "            for msg in history[-10:]:  # Only use last 10 messages\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    chat_history.append(HumanMessage(content=msg[\"content\"]))\n",
    "                else:\n",
    "                    chat_history.append(AIMessage(content=msg[\"content\"]))\n",
    "            \n",
    "            # Get the response\n",
    "            result = executor.invoke({\n",
    "                \"input\": message,\n",
    "                \"chat_history\": chat_history\n",
    "            })\n",
    "            \n",
    "            # Return the agent's response\n",
    "            return result[\"output\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def create_gradio_interface(self):\n",
    "        \"\"\"Create the Gradio web interface\"\"\"\n",
    "        with gr.Blocks(title=\"Multi-Agent Assistant\", theme=gr.themes.Soft()) as interface:\n",
    "            # Title\n",
    "            gr.Markdown(\"# 🤖 Multi-Agent Assistant with Tools\")\n",
    "            gr.Markdown(\"Chat with different AI agents that can use various tools!\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Left column - Controls\n",
    "                with gr.Column(scale=1):\n",
    "                    # Agent selector\n",
    "                    agent_dropdown = gr.Dropdown(\n",
    "                        choices=[config[\"name\"] for config in self.agent_configs],\n",
    "                        value=self.agent_configs[0][\"name\"],\n",
    "                        label=\"Select Agent\"\n",
    "                    )\n",
    "                    \n",
    "                    # Max iterations slider\n",
    "                    iterations_slider = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=10,\n",
    "                        value=self.max_iterations,\n",
    "                        step=1,\n",
    "                        label=\"Max Tool Uses\",\n",
    "                        info=\"How many times the agent can use tools\"\n",
    "                    )\n",
    "                    \n",
    "                    # Agent info display\n",
    "                    agent_info = gr.Markdown(\"\")\n",
    "                    \n",
    "                    # Text input\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"Your Message\",\n",
    "                        placeholder=\"Type your message here...\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    \n",
    "                    # Send button\n",
    "                    send_button = gr.Button(\"Send\", variant=\"primary\")\n",
    "                \n",
    "                # Right column - Chat\n",
    "                with gr.Column(scale=2):\n",
    "                    # Chat history display\n",
    "                    chatbot = gr.Chatbot(label=\"Conversation\", height=600)\n",
    "                    \n",
    "                    # Last input display\n",
    "                    last_input = gr.Textbox(label=\"Last Input\", interactive=False)\n",
    "                    \n",
    "                    # Clear button\n",
    "                    clear_button = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            # Hidden state to store conversation history\n",
    "            history_state = gr.State([])\n",
    "            \n",
    "            # Function to update agent info\n",
    "            def update_agent_info(agent_name):\n",
    "                \"\"\"Show information about the selected agent\"\"\"\n",
    "                if agent_name in self.agents:\n",
    "                    tools = self.agents[agent_name][\"tools\"]\n",
    "                    tool_names = [tool.name for tool in tools]\n",
    "                    return f\"**{agent_name}**\\n\\nAvailable tools: {', '.join(tool_names)}\"\n",
    "                return \"\"\n",
    "            \n",
    "            # Function to update max iterations\n",
    "            def update_iterations(value):\n",
    "                \"\"\"Update the max iterations for all agents\"\"\"\n",
    "                self.max_iterations = value\n",
    "                for agent_data in self.agents.values():\n",
    "                    agent_data[\"executor\"].max_iterations = value\n",
    "            \n",
    "            # Function to process messages\n",
    "            def process_message(text, agent_name, history):\n",
    "                \"\"\"Process a user message and get agent response\"\"\"\n",
    "                if not text.strip():\n",
    "                    return history, \"\", \"Please enter a message\"\n",
    "                \n",
    "                # Get agent response\n",
    "                response = self.chat_with_agent(text, agent_name, history)\n",
    "                \n",
    "                # Update history\n",
    "                new_history = history + [\n",
    "                    {\"role\": \"user\", \"content\": text},\n",
    "                    {\"role\": \"assistant\", \"content\": response}\n",
    "                ]\n",
    "                \n",
    "                # Update chat display\n",
    "                chat_display = []\n",
    "                for msg in new_history:\n",
    "                    if msg[\"role\"] == \"user\":\n",
    "                        chat_display.append((msg[\"content\"], None))\n",
    "                    else:\n",
    "                        chat_display.append((None, msg[\"content\"]))\n",
    "                \n",
    "                return new_history, \"\", text\n",
    "            \n",
    "            # Function to clear chat\n",
    "            def clear_chat():\n",
    "                \"\"\"Clear the conversation\"\"\"\n",
    "                return [], [], \"\", \"\"\n",
    "            \n",
    "            # Connect all the interface elements\n",
    "            \n",
    "            # Update agent info when selection changes\n",
    "            agent_dropdown.change(\n",
    "                update_agent_info,\n",
    "                inputs=[agent_dropdown],\n",
    "                outputs=[agent_info]\n",
    "            )\n",
    "            \n",
    "            # Update iterations when slider changes\n",
    "            iterations_slider.change(\n",
    "                update_iterations,\n",
    "                inputs=[iterations_slider]\n",
    "            )\n",
    "            \n",
    "            # Process message when send button clicked\n",
    "            send_button.click(\n",
    "                process_message,\n",
    "                inputs=[text_input, agent_dropdown, history_state],\n",
    "                outputs=[history_state, text_input, last_input]\n",
    "            )\n",
    "            \n",
    "            # Update chat display when history changes\n",
    "            history_state.change(\n",
    "                lambda h: [(m[\"content\"], None) if m[\"role\"] == \"user\" \n",
    "                          else (None, m[\"content\"]) for m in h],\n",
    "                inputs=[history_state],\n",
    "                outputs=[chatbot]\n",
    "            )\n",
    "            \n",
    "            # Clear everything when clear button clicked\n",
    "            clear_button.click(\n",
    "                clear_chat,\n",
    "                outputs=[chatbot, history_state, text_input, last_input]\n",
    "            )\n",
    "            \n",
    "            # Initialize agent info on load\n",
    "            interface.load(\n",
    "                lambda: update_agent_info(self.agent_configs[0][\"name\"]),\n",
    "                outputs=[agent_info]\n",
    "            )\n",
    "        \n",
    "        return interface\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure MCP servers (optional)\n",
    "    MCP_SERVERS = {\n",
    "        \"example_server\": {\"url\": \"http://localhost:8000/mcp\"}\n",
    "    }\n",
    "    \n",
    "    # Configure agents\n",
    "    AGENT_CONFIGS = [\n",
    "        {\n",
    "            \"name\": \"Python Coder\",\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.2,\n",
    "            \"description\": \"Helps with Python programming\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Data Analyst\", \n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"description\": \"Helps with data analysis\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Assistant\",\n",
    "            \"model\": \"gpt-4o\", \n",
    "            \"temperature\": 0.7,\n",
    "            \"description\": \"General helpful assistant\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create and launch the dashboard\n",
    "    dashboard = SimpleMultiAgentDashboard(MCP_SERVERS, AGENT_CONFIGS)\n",
    "    app = dashboard.create_gradio_interface()\n",
    "    app.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f4b51",
   "metadata": {},
   "source": [
    "Collecting workspace informationBased on my analysis of the notebooks in the `/workspaces/Summerschool_BAI_2025/1_LANGCHAIN_FUNDAMENTALS` directory, here are the big concepts covered:\n",
    "\n",
    "## 1_LANGCHAIN_FUNDAMENTALS Directory - Core Concepts\n",
    "\n",
    "### **01_basic_finance_chatbot.ipynb**\n",
    "- **LangChain Setup & Configuration**\n",
    "  - Environment setup with OpenAI API keys\n",
    "  - `ChatOpenAI` model initialization with temperature and token limits\n",
    "  - Basic LangChain imports and components\n",
    "\n",
    "- **Message Handling**\n",
    "  - `SystemMessage` and `HumanMessage` creation\n",
    "  - Basic prompt engineering for financial advice\n",
    "  - Single-turn conversations\n",
    "\n",
    "- **Financial Advisory Basics**\n",
    "  - Investment terminology explanations (dividends, compound interest)\n",
    "  - Retirement planning advice\n",
    "  - Risk tolerance concepts\n",
    "  - Basic portfolio allocation principles\n",
    "\n",
    "### **02_prompt_templates_personas.ipynb**\n",
    "- **Prompt Templates**\n",
    "  - `PromptTemplate` class for reusable prompts\n",
    "  - Variable substitution in templates\n",
    "  - `ChatPromptTemplate` for structured conversations\n",
    "\n",
    "- **Financial Advisor Personas**\n",
    "  - **Conservative Advisor**: Risk-averse, safety-focused investment strategies\n",
    "  - **Growth Advisor**: Aggressive growth, high-risk/high-reward approaches\n",
    "  - **Balanced Advisor**: Moderate risk, diversified portfolio strategies\n",
    "\n",
    "- **Template Design Patterns**\n",
    "  - Dynamic content generation\n",
    "  - Persona-based response customization\n",
    "  - Interactive advisor selection systems\n",
    "\n",
    "### **03_message_history_memory.ipynb**\n",
    "- **Conversation Memory Management**\n",
    "  - Manual message history with `HumanMessage`/`AIMessage`\n",
    "  - `MessagesPlaceholder` for dynamic conversation inclusion\n",
    "  - `ConversationBufferMemory` vs `ConversationSummaryMemory`\n",
    "\n",
    "- **Advanced Financial Tracking**\n",
    "  - `FinancialGoalTracker` class for client profile management\n",
    "  - Multi-turn conversations with context preservation\n",
    "  - Client information persistence across sessions\n",
    "\n",
    "- **Memory Types & Strategies**\n",
    "  - Buffer memory for short conversations\n",
    "  - Summary memory for long conversations with token optimization\n",
    "  - Custom memory implementations for specific business logic\n",
    "\n",
    "- **Financial Goal Management**\n",
    "  - Goal setting and tracking\n",
    "  - Progress monitoring\n",
    "  - Personalized advice based on client history\n",
    "\n",
    "## **Key Learning Progression**\n",
    "\n",
    "1. **Foundation**: Basic chatbot setup and single interactions\n",
    "2. **Customization**: Template-based responses and persona development  \n",
    "3. **Sophistication**: Memory-enabled conversations and client relationship management\n",
    "\n",
    "## **Financial Domain Applications**\n",
    "- Personal financial planning\n",
    "- Investment strategy development\n",
    "- Risk assessment and tolerance evaluation\n",
    "- Portfolio diversification analysis\n",
    "- Retirement planning guidance\n",
    "- Emergency fund recommendations\n",
    "\n",
    "## **Technical Skills Developed**\n",
    "- LangChain framework fundamentals\n",
    "- OpenAI API integration\n",
    "- Prompt engineering techniques\n",
    "- Memory management strategies\n",
    "- Object-oriented chatbot design\n",
    "- Financial advisory system architecture\n",
    "\n",
    "These notebooks provide a comprehensive foundation for building sophisticated financial advisory chatbots using LangChain, progressing from basic interactions to complex, memory-enabled financial planning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4991cf-789d-452b-8351-10b445bb8a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
